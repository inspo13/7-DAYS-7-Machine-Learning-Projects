# -*- coding: utf-8 -*-
"""Predicting Diamond prices using KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_P0CgyZ6JbE-n99pW7oROJ_Yh208Lpbk

Course Title: Applied Machine LearningCourse 
Name Prachi Bhatt
"""

#To Predicting Diamond prices using KNN Regression. USE DIAMONDS.CSV
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd #Importing the dataset

import sklearn
from sklearn import svm,preprocessing

#for importing the csv file from the directory
import os 
working_directory=os.getcwd()
print(working_directory)

path=working_directory + '/diamonds.csv'  # the dataset will be read out in read only mode.
df=pd.read_csv(path)
df # it displays the entire dataset details

dataset=df.dropna()
dataset

dataset.columns

# We are getting the data for Independent and dependent variables
X = dataset[['carat','cut','color','clarity','depth','table','x','y','z']]
y = dataset[['price']]

X

y

sns.FacetGrid(df, hue = 'cut', height = 6).map(sns.distplot, 'price').add_legend()
plt.plot()

df['cut'] = df['cut'].map(cut_dict)
df['clarity'] = df['clarity'].map(clarity_dict)
df['color'] = df['color'].map(color_dict)

df = sklearn.utils.shuffle(df, random_state = 42)
X = df.drop(['price'], axis = 1).values
X = preprocessing.scale(X)
y = df['price'].values
y = preprocessing.scale(y)

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) 
# If we are not defining test size in the method, by default it gets 25% of test size.
#test_size = 0 but default it consider 25%

X_train

X_test

y_train

y_test

# print the shapes of the new X objects
print(X_train.shape)
print(X_test.shape) 
# 25% of the actual dataset when we are not passing 
# test size into train_test_split method

# print the shapes of the new y objects
print(y_train.shape)
print(y_test.shape)

from sklearn.neighbors import KNeighborsRegressor
score = []
for k in range(1,20):   # running for different K values to know which yields the max accuracy. 
    clf = KNeighborsRegressor(n_neighbors = k,  weights = 'distance', p=1)
    clf.fit(X_train, y_train)
    score.append(clf.score(X_test, y_test ))

k_max = score.index(max(score))+1
print( "At K = {}, Max Accuracy = {}".format(k_max, max(score)*100))

clf = KNeighborsRegressor(n_neighbors = k_max,  weights = 'distance', p=1)
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test ))   
y_pred = clf.predict(X_test)

# By using predict method we can predict the model as per testing dataset
# make a prediction for an out-of-sample observation
y_pred_knn = knn.predict(X_test)

print ("Actual Values:", y_test) # actual testing dataset
print ("Predicted by KNN:", y_pred_knn) # predicted testing dataset for KNN


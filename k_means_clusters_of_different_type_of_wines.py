# -*- coding: utf-8 -*-
"""K-MEANS-clusters of different type of wines.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jch0qyW-oIJBtVtBxJi86ntcJoMRIN7x

Course Title: Applied Machine Learning
Name Prachi Bhatt
"""

#Implementation of K-Means Clustering to show clusters of different type of wines. USE WINE.CSV
# Importing some libraries
import numpy as np 
import pandas as pd

#for importing the csv file from the directory
import os 
working_directory=os.getcwd()
print(working_directory)

path=working_directory + '/Wine.csv'  # the dataset will be read out in read only mode.
df=pd.read_csv(path)
df # it displays the entire dataset details

### K-means clustering algorithm works on continous nature based data.
### so we can easily implement such algorithm

### It is an unsupervised learning model. It means that we don't know where data are lying
### or output is unknown due to unlabel data

# total size of the dataset
df.shape

# print or display total number of columns
df.columns

# The info () method gives use the complete details of the dataframe
# like total number of values holding by each column and row, null or non-null etc.
df.info()

# Describe () method describes the mathematical function value
df.describe()
# This method works only on numerical data (columns which are having only numerical values)

# checking null values
df.isnull().sum()

# So far we have prepared and understand about the dataset 
# We must be happy here to know we have complete dataset which does not belong to any null value.

import matplotlib.pyplot as plt

#There is a strong co-relation between Total_Phenols and Flavanoids
plt.figure(1 , figsize = (15 , 7))
plt.title('Scatter Graph of Total_Phenols V/S Flavanoids', fontsize = 20)
plt.xlabel('Total_Phenols')
plt.ylabel('Flavanoids')
plt.scatter( x = 'Total_Phenols', y = 'Flavanoids', data = df, s = 50)
plt.show()

#Based on Total_Phenols and Flavanoids, the cluters will be generated.
# Let us first determine the kth value (Total cluster size)

# As we know we must have cluster size first before making clusters on the dataset.
# Now its time to decide cluster size 
# we need to import 'kmeans' class from 'cluster' library from the 'sklearn' package

from sklearn.cluster import KMeans

# Deciding K value
# It is necessary to decide k value before centroids as per the K-means algorithm
# we have chosen age and Annual Income (k$) columns
X1 = df[['Total_Phenols' , 'Flavanoids']].iloc[: , :].values
#X1 would be our 2D array variable.

print (X1) # Total Phenols and Flavanoids

# we need to decide class of clusters based on similarity features
# inertia variable treats as 'means sum of the squared distances'
inertia = []

# we need to use a loop upto 22 times and by this we can get the optimum value of kth.
for n in range(1 , 50):
    # we have kmeans method which is a part of kmeans class.
    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 , max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm='elkan') )
    

    # k-means++ = select initial cluster center for the k-means clustering in a smart way to speed up 
    # convergece and instead of k-means++ we can use 'random'
    
    #n_init = no. of time k-means algorithm will be run with different centroid seeds.

    #tol = relative tolerance

    # algorithm = auto / full / elkan
    # elkan = The elkan variation is more efficient on data with well defined clusters by 
    # using the triangle inequality

    # we are fitting the model based on defined parameters as mentioned above
    algorithm.fit(X1) # X1 contains the data
    
    # inertia keeps the list of means sum of the squared distances
    inertia.append(algorithm.inertia_)

# the inertia variable holds the sum of squared distances as per total loop size i.e; 50
print(inertia)

# we need to display the information to get optimum kth value
# We are plotting a graph to know the 'elbowing' value of kth to get optimum value

plt.figure(1 , figsize = (15 ,6))
plt.plot(np.arange(1 , 50) , inertia , 'o') # small circle used to point out the data
plt.plot(np.arange(1 , 50) , inertia , '-', alpha = 0.5) # this will be used to display each point by line style.
plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
plt.show()

# You have been observed here that k=3 is the optimum value
# Applying KMeans for k or n=3
algorithm = (KMeans(n_clusters = 3 ,init='k-means++', n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm='elkan') )
# again fit the model towards k = 3
algorithm.fit(X1)

# this is required to get centroid to make connection between cetroid and datapoint
# label is deciding which data point is belong to which cluster.
# if you have decided that k=3, it means we have range of the clusters from 0 to 2
# label of the datapoints towards the cluster index value
labels1 = algorithm.labels_

# its time to decide centroid of the clusters and we got k=3 (cluster size = 3 and indexed by 0 to 2)
centroids1 = algorithm.cluster_centers_

labels1

centroids1 # this contains the coordinates values

# So far we have used x1 variable which contains Total_Phenols and Flavanoids
# and as we know its is a 2D array.

# we need to display our information in cluster pattern basis 
# for doing this we need to transform 2D to one-dimension array by using ravel () method.

h = 0.02 
# this is required to preserve uniformity between x and y datapoints.
# when you are transforming image from one-dimension to another dimernsion.

# Suppose you have a 3D image and want to transform into 2D image 
# keep in mind you want to preserve the coordinates values of x and y 
# after removing or adjusting z-dimension value
# so our actual values of x and y coordinates should not be disturbed.

x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1 # Total_Phenols or 0th index
y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1 #Flavanoids

# we need to use meshgrid method from numpy library to get or used to create pairs of 
# coordinates between a range (min and max as we have seen above code (x_min, x_max , y_min and y_max))
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

print (xx)
print (yy)

# for clustering we need to make our graph into dimension pattern to get proper clusters
Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 
# The above code will decide the coordinates values with respect to the xx and yy variables.

plt.figure(1 , figsize = (15 , 7) )

# it will clear the graph from existing pattern
plt.clf() # clear the frame because plt variable holds some information into its memory.

Z = Z.reshape(xx.shape) # it is required towards the x and we are changinng its size towards the x.

plt.imshow(Z , interpolation='nearest', 
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')

plt.scatter( x = 'Total_Phenols', y = 'Flavanoids', data = df, c = labels1, s = 50)
plt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 100 , c = 'red' , alpha = 0.5)
plt.ylabel('Flavanoids') , plt.xlabel('Total_Phenols')
plt.show()

# Applying KMeans for k=7
algorithm = (KMeans(n_clusters = 7, init='k-means++', n_init = 10, max_iter=300, 
                        tol=0.0001, random_state= 111 , algorithm='elkan'))
algorithm.fit(X1)
labels1 = algorithm.labels_
centroids1 = algorithm.cluster_centers_
centroids1

h = 0.02
x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1
y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])

plt.figure(1 , figsize = (15 , 7) )
plt.clf()
Z = Z.reshape(xx.shape)
plt.imshow(Z , interpolation='nearest', 
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')

plt.scatter( x = 'Total_Phenols', y = 'Flavanoids', data = df, c = labels1, s = 100)
plt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)
plt.ylabel('Flavanoids') , plt.xlabel('Total_Phenols')
plt.show()

X3 = df[['Total_Phenols' , 'Flavanoids' ,'Alcohol']].iloc[: , :].values
inertia = []
for n in range(1 , 11):
    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, 
                        tol=0.0001, random_state= 111, algorithm='elkan'))
    algorithm.fit(X3)
    inertia.append(algorithm.inertia_)

plt.figure(1 , figsize = (15 ,6))
plt.plot(np.arange(1 , 11) , inertia , 'o')
plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
plt.show()

algorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm='elkan') )
algorithm.fit(X3)
labels3 = algorithm.labels_
centroids3 = algorithm.cluster_centers_

y_kmeans = algorithm.fit_predict(X3)
df['cluster'] = pd.DataFrame(y_kmeans)
df.head()


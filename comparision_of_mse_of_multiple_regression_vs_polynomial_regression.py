# -*- coding: utf-8 -*-
"""Comparision of MSE of Multiple Regression vs Polynomial Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ekxnXnfp2184W_Y8rxGIdkB9jK9whE5T
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import LabelEncoder
df = pd.read_csv('/content/winequality-red.csv')
df.head()

#X = df[['fixed acidity', 'volatile acidity','citric acid','residual sugar','chlorides', 'free sulfur dioxide','total sulfur dioxide','density','pH' , 'sulphates','alcohol']] #independent variables
#y = df['quality'] #dependent variable
#READING MISSING VALUES IN EACH COLUMN
df.isna().sum()

#INDEPENDENT VARIABLE
x = df.iloc[:, :-1].values
#DEPENDENT VARIABLE(QUALITY)
y = df.iloc[:, -1].values

#SPLITTING DATA INTO TRAINING AND TESTING DATASET
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state = 19)

#TRAINING THE MODEL
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 2)
x_poly = poly_reg.fit_transform(x_train)
linear_reg = LinearRegression()
linear_reg.fit(x_poly, y_train)

#GETTING PREDICTIONS
y_pred = linear_reg.predict(poly_reg.transform(x_test))

#CALCULATING ACCURACY
from sklearn.metrics import r2_score, mean_squared_error
print("MSE is ",mean_squared_error(y_test, y_pred))
print("R2 score is ",r2_score(y_test, y_pred))

"""Comparision of MSE of Multiple Regression vs Polynomial Regression:

It is clearly visible that MSE of Multiple regression = 0.42 While MSE of Polynomial Regression = 0.4090834281195525. Thus,we arrive at a conclusion that Polynomial Regression is better than Multiple
regression as it has low MSE.
"""

